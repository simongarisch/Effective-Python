{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider module scoped code to configure deployment environments\n",
    "The best way to do this is to override parts of your program at startup time to provide different functionality depending on your deployment environment. However, once your deployment environments get complicated, you should consider moving them out of Python constants (like TESTING) and into dedicated configuration files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use repr strings for debugging output\n",
    "When you're debugging with print, you should repr the value before printing to ensure that any difference in types is clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(repr(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'5'\n"
     ]
    }
   ],
   "source": [
    "print(repr('5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classes we can define our own repr special method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BetterClass(1, 2)\n"
     ]
    }
   ],
   "source": [
    "class BetterClass(object):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return(\"BetterClass(%d, %d)\" % (self.x, self.y))\n",
    "    \n",
    "obj = BetterClass(1, 2)\n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or where you don't have control over the class definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 1, 'y': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test everything with unittest\n",
    "You should always test your code, regardless of which language it's written in. The unittest module is popular, but Python users appear to be using pytest more these days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SomeClass(object):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def sumXY(self):\n",
    "        return(self.x + self.y)\n",
    "    \n",
    "    def multiplyXY(self):\n",
    "        return(self.x * self.y)\n",
    "    \n",
    "\n",
    "someObj = SomeClass(2, 3)\n",
    "someObj.sumXY(), someObj.multiplyXY()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setUpClass finished!\n",
      "\n",
      "setUp finished!\n",
      "test_init finished!\n",
      "tearDown finished!\n",
      "\n",
      "setUp finished!\n",
      "test_multiplyXY finished!\n",
      "tearDown finished!\n",
      "\n",
      "setUp finished!\n",
      "test_sumXY finished!\n",
      "tearDown finished!\n",
      "\n",
      "tearDownClass finished!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.009s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestSomeClass(unittest.TestCase):\n",
    "        \n",
    "    # setUp and tearDown methods run before and after each test respectively\n",
    "    def setUp(self):\n",
    "        self.someObj = SomeClass(2, 3)\n",
    "        print(\"setUp finished!\")\n",
    "    \n",
    "    def tearDown(self):\n",
    "        del self.someObj\n",
    "        print(\"tearDown finished!\")\n",
    "        print(\"\")\n",
    "        \n",
    "    # the setUpClass and tearDownClass class methods are run when the class is created and destroyed respectively\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        print(\"setUpClass finished!\")\n",
    "        print(\"\")\n",
    "    \n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        print(\"tearDownClass finished!\")\n",
    "        print(\"\")\n",
    "    \n",
    "    def test_init(self):\n",
    "        self.assertEqual(self.someObj.x, 2)\n",
    "        self.assertEqual(self.someObj.y, 3)\n",
    "        print(\"test_init finished!\")\n",
    "    \n",
    "    def test_sumXY(self):\n",
    "        self.assertEqual(self.someObj.sumXY(), 5)\n",
    "        print(\"test_sumXY finished!\")\n",
    "    \n",
    "    def test_multiplyXY(self):\n",
    "        self.assertEqual(self.someObj.multiplyXY(), 6)\n",
    "        print(\"test_multiplyXY finished!\")\n",
    "    \n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        unittest.main() # this usually works, but it won't in the Jupyter notebook\n",
    "    except:\n",
    "        unittest.main(argv=[\"first-arg-is-ignored\"], exit=False) # run this instead\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note:\n",
    "* We can run tests by inheriting from unittest.TestCase\n",
    "* Testing methods need to start with the name 'test...'\n",
    "* There are setUp and tearDown methods that can run before and after each test method. These assist in the actions of one test not polluting another.\n",
    "* setUpClass and tearDownClass class methods can also be very useful. Creating persistent objects that are time consuming is a good candidate for setUpClass, whereas tearDownClass could do a clean up and delete any files that were created as part of the testing process, for example.\n",
    "* pytest appears to be more popular that unittest at the time of writing.\n",
    "* unittest is often used in conjunction with nose and coverage.\n",
    "* There is no guarantee of the order in which tests will run, but there are ways around this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_proxy({'__call__': <function unittest.case.__call__>,\n",
       "            '__dict__': <attribute '__dict__' of 'TestCase' objects>,\n",
       "            '__doc__': \"A class whose instances are single test cases.\\n\\n    By default, the test code itself should be placed in a method named\\n    'runTest'.\\n\\n    If the fixture may be used for many test cases, create as\\n    many test methods as are needed. When instantiating such a TestCase\\n    subclass, specify in the constructor arguments the name of the test method\\n    that the instance is to execute.\\n\\n    Test authors should subclass TestCase for their own tests. Construction\\n    and deconstruction of the test's environment ('fixture') can be\\n    implemented by overriding the 'setUp' and 'tearDown' methods respectively.\\n\\n    If it is necessary to override the __init__ method, the base class\\n    __init__ method must always be called. It is important that subclasses\\n    should not change the signature of their __init__ method, since instances\\n    of the classes are instantiated automatically by parts of the framework\\n    in order to be run.\\n\\n    When subclassing TestCase, you can set these attributes:\\n    * failureException: determines which exception will be raised when\\n        the instance's assertion methods fail; test methods raising this\\n        exception will be deemed to have 'failed' rather than 'errored'.\\n    * longMessage: determines whether long messages (including repr of\\n        objects used in assert methods) will be printed on failure in *addition*\\n        to any explicit message passed.\\n    * maxDiff: sets the maximum length of a diff in failure messages\\n        by assert methods using difflib. It is looked up as an instance\\n        attribute so can be configured by individual tests if required.\\n    \",\n",
       "            '__eq__': <function unittest.case.__eq__>,\n",
       "            '__hash__': <function unittest.case.__hash__>,\n",
       "            '__init__': <function unittest.case.__init__>,\n",
       "            '__module__': 'unittest.case',\n",
       "            '__ne__': <function unittest.case.__ne__>,\n",
       "            '__repr__': <function unittest.case.__repr__>,\n",
       "            '__str__': <function unittest.case.__str__>,\n",
       "            '__weakref__': <attribute '__weakref__' of 'TestCase' objects>,\n",
       "            '_addSkip': <function unittest.case._addSkip>,\n",
       "            '_baseAssertEqual': <function unittest.case._baseAssertEqual>,\n",
       "            '_classSetupFailed': False,\n",
       "            '_deprecate': <function unittest.case._deprecate>,\n",
       "            '_diffThreshold': 65536,\n",
       "            '_formatMessage': <function unittest.case._formatMessage>,\n",
       "            '_getAssertEqualityFunc': <function unittest.case._getAssertEqualityFunc>,\n",
       "            '_truncateMessage': <function unittest.case._truncateMessage>,\n",
       "            'addCleanup': <function unittest.case.addCleanup>,\n",
       "            'addTypeEqualityFunc': <function unittest.case.addTypeEqualityFunc>,\n",
       "            'assertAlmostEqual': <function unittest.case.assertAlmostEqual>,\n",
       "            'assertAlmostEquals': <function unittest.case.assertAlmostEqual>,\n",
       "            'assertDictContainsSubset': <function unittest.case.assertDictContainsSubset>,\n",
       "            'assertDictEqual': <function unittest.case.assertDictEqual>,\n",
       "            'assertEqual': <function unittest.case.assertEqual>,\n",
       "            'assertEquals': <function unittest.case.assertEqual>,\n",
       "            'assertFalse': <function unittest.case.assertFalse>,\n",
       "            'assertGreater': <function unittest.case.assertGreater>,\n",
       "            'assertGreaterEqual': <function unittest.case.assertGreaterEqual>,\n",
       "            'assertIn': <function unittest.case.assertIn>,\n",
       "            'assertIs': <function unittest.case.assertIs>,\n",
       "            'assertIsInstance': <function unittest.case.assertIsInstance>,\n",
       "            'assertIsNone': <function unittest.case.assertIsNone>,\n",
       "            'assertIsNot': <function unittest.case.assertIsNot>,\n",
       "            'assertIsNotNone': <function unittest.case.assertIsNotNone>,\n",
       "            'assertItemsEqual': <function unittest.case.assertItemsEqual>,\n",
       "            'assertLess': <function unittest.case.assertLess>,\n",
       "            'assertLessEqual': <function unittest.case.assertLessEqual>,\n",
       "            'assertListEqual': <function unittest.case.assertListEqual>,\n",
       "            'assertMultiLineEqual': <function unittest.case.assertMultiLineEqual>,\n",
       "            'assertNotAlmostEqual': <function unittest.case.assertNotAlmostEqual>,\n",
       "            'assertNotAlmostEquals': <function unittest.case.assertNotAlmostEqual>,\n",
       "            'assertNotEqual': <function unittest.case.assertNotEqual>,\n",
       "            'assertNotEquals': <function unittest.case.assertNotEqual>,\n",
       "            'assertNotIn': <function unittest.case.assertNotIn>,\n",
       "            'assertNotIsInstance': <function unittest.case.assertNotIsInstance>,\n",
       "            'assertNotRegexpMatches': <function unittest.case.assertNotRegexpMatches>,\n",
       "            'assertRaises': <function unittest.case.assertRaises>,\n",
       "            'assertRaisesRegexp': <function unittest.case.assertRaisesRegexp>,\n",
       "            'assertRegexpMatches': <function unittest.case.assertRegexpMatches>,\n",
       "            'assertSequenceEqual': <function unittest.case.assertSequenceEqual>,\n",
       "            'assertSetEqual': <function unittest.case.assertSetEqual>,\n",
       "            'assertTrue': <function unittest.case.assertTrue>,\n",
       "            'assertTupleEqual': <function unittest.case.assertTupleEqual>,\n",
       "            'assert_': <function unittest.case.assertTrue>,\n",
       "            'countTestCases': <function unittest.case.countTestCases>,\n",
       "            'debug': <function unittest.case.debug>,\n",
       "            'defaultTestResult': <function unittest.case.defaultTestResult>,\n",
       "            'doCleanups': <function unittest.case.doCleanups>,\n",
       "            'fail': <function unittest.case.fail>,\n",
       "            'failIf': <function unittest.case.deprecated_func>,\n",
       "            'failIfAlmostEqual': <function unittest.case.deprecated_func>,\n",
       "            'failIfEqual': <function unittest.case.deprecated_func>,\n",
       "            'failUnless': <function unittest.case.deprecated_func>,\n",
       "            'failUnlessAlmostEqual': <function unittest.case.deprecated_func>,\n",
       "            'failUnlessEqual': <function unittest.case.deprecated_func>,\n",
       "            'failUnlessRaises': <function unittest.case.deprecated_func>,\n",
       "            'failureException': AssertionError,\n",
       "            'id': <function unittest.case.id>,\n",
       "            'longMessage': False,\n",
       "            'maxDiff': 640,\n",
       "            'run': <function unittest.case.run>,\n",
       "            'setUp': <function unittest.case.setUp>,\n",
       "            'setUpClass': <classmethod at 0x3f81bb8>,\n",
       "            'shortDescription': <function unittest.case.shortDescription>,\n",
       "            'skipTest': <function unittest.case.skipTest>,\n",
       "            'tearDown': <function unittest.case.tearDown>,\n",
       "            'tearDownClass': <classmethod at 0x3f81be8>})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.TestCase.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider interactive debugging with pdb\n",
    "To initiate the debugger, all you have to do is import the pdb built-in module and run its set_trace function. As the debugger runs we can print out the value of variables, hit 'n' to go to the next line and 'c' to continue running the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-15-350461409232>(4)add_to_life_universe_everything()\n",
      "-> answer += x\n",
      "(Pdb) answer\n",
      "42\n",
      "(Pdb) x\n",
      "12\n",
      "(Pdb) n\n",
      "> <ipython-input-15-350461409232>(6)add_to_life_universe_everything()\n",
      "-> return answer\n",
      "(Pdb) answer\n",
      "54\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_to_life_universe_everything(x):\n",
    "    answer = 42\n",
    "    import pdb; pdb.set_trace()\n",
    "    answer += x\n",
    "    \n",
    "    return answer\n",
    "\n",
    "add_to_life_universe_everything(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile before optimising\n",
    "The dynamic nature of Python causes surprising behaviors in its runtime performance. Operations you might assume are slow are actually very fast. The best approach is to ignore your intuition and directly measure the performance of your program before you try to optimise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insertion_sort(data):\n",
    "    result = []\n",
    "    for value in data:\n",
    "        insert_value(result, value)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_value(array, value):\n",
    "    for i, existing in enumerate(array):\n",
    "        if existing > value:\n",
    "            array.insert(i, value)\n",
    "            return\n",
    "    array.append(value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "max_size = 10**4\n",
    "data = [randint(0, max_size) for _ in range(max_size)]\n",
    "test = lambda: insertion_sort(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python provides two built-in profilers, one that is pure Python (profile) and another that is a C-extension module (cProfile) which is the better option. The pure python alternative imposes a high overhead that will skew the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://docs.python.org/2/library/profile.html\n",
    "import cProfile\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.runcall(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         20003 function calls in 0.997 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    10000    0.980    0.000    0.995    0.000 <ipython-input-23-6c0b42aafbe9>:1(insert_value)\n",
      "     9986    0.014    0.000    0.014    0.000 {method 'insert' of 'list' objects}\n",
      "        1    0.003    0.003    0.997    0.997 <ipython-input-22-e5e99c0030b8>:1(insertion_sort)\n",
      "        1    0.000    0.000    0.997    0.997 <ipython-input-24-1054ed032f49>:5(<lambda>)\n",
      "       14    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats instance at 0x00000000058A8EC8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pstats\n",
    "\n",
    "stats = pstats.Stats(profiler)\n",
    "#stats.sort_stats(\"cumulative\") # https://docs.python.org/2/library/profile.html#pstats.Stats.sort_stats\n",
    "stats.sort_stats(\"tottime\")\n",
    "stats.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So most of the time is spent in the insert_value function which is inefficient. Replacing this with the bisect built-in module: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "\n",
    "def insert_value(array, value):\n",
    "    i = bisect_left(array, value)\n",
    "    array.insert(i, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "profiler = cProfile.Profile()\n",
    "profiler.runcall(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         30003 function calls in 0.024 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "    10000    0.012    0.000    0.012    0.000 {method 'insert' of 'list' objects}\n",
      "    10000    0.005    0.000    0.005    0.000 {_bisect.bisect_left}\n",
      "    10000    0.004    0.000    0.022    0.000 <ipython-input-37-9e3ac1f6182f>:3(insert_value)\n",
      "        1    0.003    0.003    0.024    0.024 <ipython-input-22-e5e99c0030b8>:1(insertion_sort)\n",
      "        1    0.000    0.000    0.024    0.024 <ipython-input-24-1054ed032f49>:5(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats instance at 0x00000000058A81C8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = pstats.Stats(profiler)\n",
    "stats.sort_stats(\"tottime\")\n",
    "stats.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Ordered by: internal time\n",
      "\n",
      "Function                                           was called by...\n",
      "                                                       ncalls  tottime  cumtime\n",
      "{method 'insert' of 'list' objects}                <-   10000    0.012    0.012  <ipython-input-37-9e3ac1f6182f>:3(insert_value)\n",
      "{_bisect.bisect_left}                              <-   10000    0.005    0.005  <ipython-input-37-9e3ac1f6182f>:3(insert_value)\n",
      "<ipython-input-37-9e3ac1f6182f>:3(insert_value)    <-   10000    0.004    0.022  <ipython-input-22-e5e99c0030b8>:1(insertion_sort)\n",
      "<ipython-input-22-e5e99c0030b8>:1(insertion_sort)  <-       1    0.003    0.024  <ipython-input-24-1054ed032f49>:5(<lambda>)\n",
      "<ipython-input-24-1054ed032f49>:5(<lambda>)        <-\n",
      "{method 'disable' of '_lsprof.Profiler' objects}   <-\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats instance at 0x00000000058A81C8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.print_callers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tracemalloc to understand memory usage and leaks\n",
    "Here we have a program that wastes memory by keeping references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40387 objects before \n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "found_objects = gc.get_objects()\n",
    "print(\"%d objects before \" % len(found_objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# waste memory here\n",
    "from random import randint\n",
    "\n",
    "class SomeObj(object):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "someDict = {}\n",
    "for i in range(1000000):\n",
    "    someDict[i] = SomeObj(randint(0, 1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(someDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1040432 objects after \n"
     ]
    }
   ],
   "source": [
    "found_objects = gc.get_objects()\n",
    "print(\"%d objects after \" % len(found_objects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with gc.get_objects is that it doesn't tell you anything about how the objects were allocated. Python 3.4 introduces a new tracemalloc built-in module for solving this problem. See the docs for more details: http://pytracemalloc.readthedocs.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
